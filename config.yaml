# HYPERPARAMETERS --------------------
batch_size: 128
epochs: 3
lr: 5e-2
verbose: True
backward_method: DFA # possible options "DFA" if not Backprop is used

# -------------------------------------
# Regularization Methods ------------------------------------
#
# available_regularizaiton_methods:
#     - Dropout,
#     - BatchNorm1D
#     - LayerNorm
#     - Weight Decay
#     - L1 Regularization
#     - Learning Rate Scheduling
#     - Early Stoppping'
# ]

p_drop: 0.0 # Dropout probability. If set to 0 than no dropout will be applied
use_BatchNorm1D: False # If True apply batch norm
use_LayerNorm1D: False # If True apply layer norm
l1_regularization_lambda: 0 # L1 regularization lambda value. If set to 0 then no L1 regularization applied
l2_regularization_lambda: 0 # L1 regularization lambda value. If set to 0 then no L1 regularization applied
weight_decay: 0 # Weight Decay. If set to 0 than it is regular SGD with no weight decay (set to something btw [1e-1, 1e-7])
lr_schedular: {
    use_lr_schedular: True, # If True, torch.optim.lr_schedular.ConstantLR is used
    factor: 0.5, # The number we multiply learning rate until the milestone
    total_iters: 5, # The number of steps that the scheduler decays the learning rate
  }
# ------------------------------------
